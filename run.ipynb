{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SocialGrep/one-million-reddit-jokes\", split='train[:30%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['score'] = pd.to_numeric(df['score'])\n",
    "data_df = df.sort_values(by=\"score\", ascending=True, ignore_index=True)\n",
    "value1 = data_df.iloc[150000]\n",
    "distribution = data_df['score'].value_counts()\n",
    "length = 300000\n",
    "# we want 4 different subcategories\n",
    "value1 = round(length * (1/4))\n",
    "value2 = round(length * (2/4))\n",
    "value3 = round(length * (3/4))\n",
    "total = 0\n",
    "start = 0\n",
    "end = 0\n",
    "end2 = 0\n",
    "end3 = 0\n",
    "difference = np.Inf\n",
    "difference2 = np.Inf\n",
    "difference3 = np.Inf\n",
    "\n",
    "for value, count in distribution.items():\n",
    "    total += count\n",
    "    if abs(value1 - total) < difference:\n",
    "        difference = abs(value1 - total)\n",
    "        end = value\n",
    "    if abs(value2 - total) < difference2:\n",
    "        difference2 = abs(value2 - total)\n",
    "        end2 = value\n",
    "    if abs(value3 - total) < difference3:\n",
    "        difference3 = abs(value3 - total)\n",
    "        end3 = value\n",
    "\n",
    "print([end, end2, end3])\n",
    "#bounds are (0, 0), (1, 2), (3, 8), (8, INF)\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "import cluster_functions as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[(data_df['score']>8)]\n",
    "phrase_model = Phrases([doc.split() for doc in data_df.loc[:,\"title\"].values], \n",
    "                        min_count = 2, \n",
    "                        threshold = 0.7, \n",
    "                        connector_words = ENGLISH_CONNECTOR_WORDS, scoring = \"npmi\"\n",
    "                        )\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phrase_model.export_phrases().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[:,\"title\"] = [\" \".join(phrase_model[sentence.split()]) for sentence in data_df.loc[:,\"title\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_extraction.text.CountVectorizer(input='content', \n",
    "                                                encoding='utf-8', \n",
    "                                                decode_error='ignore', \n",
    "                                                lowercase=True, \n",
    "                                                tokenizer = None,\n",
    "                                                ngram_range=(1, 1), \n",
    "                                                analyzer='word', \n",
    "                                                max_features=500,   #Choose number of future stopwords\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn first fits then transforms\n",
    "features.fit(data_df.loc[:,\"title\"].values)\n",
    "#The most frequent words can be found in the dictionary of vocabulary items\n",
    "stopwords = list(features.vocabulary_.keys())\n",
    "print(stopwords)\n",
    "print(\"ABOVE: Frequent words to exclude\")\n",
    "\n",
    "#Create a loop to continue clustering until the largest category is not too big\n",
    "main_topic = data_df    #Initialize main topic\n",
    "cluster_prefix = \"Topic\"     #Start with root topics\n",
    "holder = []\n",
    "starting_length = len(data_df)\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(cp)\n",
    "while True:\n",
    "\n",
    "    #Run clustering\n",
    "    counter += 1\n",
    "    main_topic, other_topics, most_frequent = cp.cluster(stopwords, main_topic, cluster_prefix)\n",
    "    cluster_prefix = str(most_frequent)\n",
    "\n",
    "    #Check stopping conditions, no topic over 20% of documents\n",
    "    if len(main_topic)/len(data_df) < 0.20:\n",
    "        holder.append(other_topics)\n",
    "        holder.append(main_topic)\n",
    "        break\n",
    "\n",
    "    #Keep going\n",
    "    else:\n",
    "        holder.append(other_topics)\n",
    "        print(\"Continuing after round \" + str(counter), \"Current: \", len(main_topic), \"Total: \", starting_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat(holder)\n",
    "data_df.sort_values(\"Topic\", inplace = True)\n",
    "data_df\n",
    "# #Reorder columns\n",
    "data_df = data_df.loc[:,[\"title\", \"score\", \"Topic\"]]\n",
    "# print(data_df)\n",
    "print(data_df.Topic.value_counts())\n",
    "\n",
    "# #Saves\n",
    "data_df.to_csv(\"Jokes3_by_topic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df2 = data_df.loc[:,[\"title\", \"score\"]]\n",
    "main_topic2 = data_df2    #Initialize main topic\n",
    "cluster_prefix = \"Syntax\"     #Start with root topics\n",
    "holder2 = []\n",
    "starting_length2 = len(data_df2)\n",
    "counter = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    #Run clustering\n",
    "    counter += 1\n",
    "    main_topic2, other_topics, most_frequent = cp.cluster2(main_topic2, cluster_prefix)\n",
    "    cluster_prefix = str(most_frequent)\n",
    "\n",
    "    #Check stopping conditions, no topic over 20% of documents\n",
    "    if len(main_topic2)/len(data_df) < 0.35:\n",
    "        holder2.append(other_topics)\n",
    "        holder2.append(main_topic2)\n",
    "        break\n",
    "\n",
    "    #Keep going\n",
    "    else:\n",
    "        holder2.append(other_topics)\n",
    "        print(\"Continuing after round \" + str(counter), \"Current: \", len(main_topic2), \"Total: \", starting_length2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df2 = pd.concat(holder2)\n",
    "data_df2.sort_values(\"Syntax\", inplace = True)\n",
    "data_df2\n",
    "# #Reorder columns\n",
    "data_df2 = data_df2.loc[:,[\"title\", \"score\", \"Syntax\"]]\n",
    "# print(data_df)\n",
    "print(data_df2.Syntax.value_counts())\n",
    "\n",
    "# #Saves\n",
    "data_df2.to_csv(\"Jokes3_by_structure.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
